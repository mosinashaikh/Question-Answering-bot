# -*- coding: utf-8 -*-
"""Chat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tHFKlR__RmgR3sNW8g3dMesDcGRWKzY9
"""

pip install nltk

pip install newspaper3k

from newspaper import Article
import random
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import numpy as np
import warnings

warnings.filterwarnings("ignore")

nltk.download("punkt",quiet=True)
nltk.download("wordnet",quiet=True)

#article=Article("https://www.mayoclinic.org/diseases-conditions/chronic-kidney-disease/symptoms-causes/syc-20354521")
#article.download()
#article.parse()
#article.nlp()
#corpus=article.text
corpus="Steve was born in Tokyo, Japan in 1950. He moved to London with his parents when he was 5 years old. Steve started school there and his father began work at the hospital. His mother was a house wife and he had four brothers.He lived in England for 2 years then moved to Amman, Jordan where he lived there for 10 years. Steve then moved to Cyprus to study at the Mediterranean University. Unfortunately, he did not succeed and returned to Jordan. His parents were very unhappy so he decided to try in America.He applied to many colleges and universities in the States and finally got some acceptance offers from them. He chose Wichita State University in Kansas. His major was Bio-medical Engineering. He stayed there for bout six months and then he moved again to a very small town called Greensboro to study in a small college."
print(corpus)

text=corpus
sent_token= nltk.sent_tokenize(text)

print(sent_token)

remove_punct_dict= dict( (punct,None) for punct in string.punctuation)
print(string.punctuation)

print(remove_punct_dict)

def LemNormalize(text):
  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))

print(LemNormalize(text))

GREETING_INPUT=["hi","hello","hola","hey"]

GREETING_RESPONSE=["howdy","hello","hey","whta's good"]
def greeting(sentence):
  for word in sentence.split():
    if word.lower()in GREETING_INPUT:
      return random.choice(GREETING_RESPONSE)

def response(user_response):
  #user_response="What are the causes"
  #user_response=user_response.lower()
  #print(user_response)

  robo_response=""

  sent_token.append(user_response)
  #print(sent_token)

  TfidVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words="english")

  tfidf=TfidVec.fit_transform(sent_token)
  #print(tfidf)

  vals=cosine_similarity(tfidf[-1],tfidf)
  #print(vals)

  idx=vals.argsort()[0][-2]

  flat=vals.flatten()
  flat.sort()
  score=flat[-2]
  #print(score)

  if(score==0):
    robo_reponse=robo_response+"I apologize, I dont understand."
  else:
    robo_reponse=robo_response+sent_token[idx]  

  print(robo_reponse)
  sent_token.remove(user_response)
  return(robo_response)

flag=True
print("I am question anwering bot.Ask me anything from the given passage. If you want to exit type Bye!")
while(flag==True):
  user_response=input()
  user_response=user_response.lower()
  if(user_response!= "Bye!"):
    if(user_response=="thanks" or user_response=="thank you"):
      flag=False
      print("DOCBOT: You are welcome!")
    else:
      if(greeting(user_response)!=None):
        print("DOCBOT: "+greeting(user_response))  
      else:
        print("DOCBOT:"+response(user_response))

  else:
    flag=False
    print("DOCBOT: Chat with you later!")